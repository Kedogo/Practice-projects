{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e14041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset URL\n",
    "url = \"https://raw.githubusercontent.com/dsrscientist/dataset1/master/census_income.csv\"\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01681120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset URL\n",
    "url = \"https://raw.githubusercontent.com/dsrscientist/dataset1/master/census_income.csv\"\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display basic statistics of numerical features\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\nFirst Few Rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Dataset URL\n",
    "url = \"https://raw.githubusercontent.com/dsrscientist/dataset1/master/census_income.csv\"\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Handling missing values\n",
    "# For simplicity, let's fill missing values in numerical columns with their mean\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[numerical_columns] = imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "# For categorical columns, let's fill missing values with the most frequent value\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_columns] = imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Display the first few rows of the preprocessed DataFrame\n",
    "print(\"\\nPreprocessed Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics of the preprocessed DataFrame\n",
    "print(\"\\nBasic Statistics of Preprocessed Data:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Optionally, you may scale numerical features if necessary\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Display basic statistics of the scaled DataFrame\n",
    "print(\"\\nBasic Statistics of Scaled Data:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Dataset URL\n",
    "url = \"https://raw.githubusercontent.com/dsrscientist/dataset1/master/census_income.csv\"\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Handling missing values\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[numerical_columns] = imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_columns] = imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Feature Engineering\n",
    "# Create a new feature by concatenating 'education' and 'occupation'\n",
    "df['edu_occupation'] = df['education'] + '_' + df['occupation']\n",
    "\n",
    "# Display the first few rows of the DataFrame with the new feature\n",
    "print(\"\\nDataFrame with New Feature:\")\n",
    "print(df.head())\n",
    "\n",
    "# Optionally, you may scale numerical features if necessary\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Display basic statistics of the scaled DataFrame\n",
    "print(\"\\nBasic Statistics of Scaled Data:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Dataset URL\n",
    "url = \"https://raw.githubusercontent.com/dsrscientist/dataset1/master/census_income.csv\"\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Handling missing values\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[numerical_columns] = imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_columns] = imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Feature Engineering\n",
    "df['edu_occupation'] = df['education'] + '_' + df['occupation']\n",
    "\n",
    "# Optionally, you may scale numerical features if necessary\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop('income', axis=1)  # Features\n",
    "y = df['income']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "print(\"\\nTraining set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred_logistic = logistic_model.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "y_pred_decision_tree = decision_tree_model.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "y_pred_random_forest = random_forest_model.predict(X_test)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(f\"Model: {type(model).__name__}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "evaluate_model(logistic_model, y_test, y_pred_logistic)\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "evaluate_model(decision_tree_model, y_test, y_pred_decision_tree)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "evaluate_model(random_forest_model, y_test, y_pred_random_forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95896f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Choose a classification model (Logistic Regression in this example)\n",
    "chosen_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the chosen model on the training set\n",
    "chosen_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ebf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Use the trained model to make predictions on the testing set\n",
    "y_pred = chosen_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Display the detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa83e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# Choose a classification model (Logistic Regression in this example)\n",
    "chosen_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(chosen_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters on the entire training set\n",
    "best_model = LogisticRegression(random_state=42, **best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model on the testing set\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Display the evaluation metrics for the best model\n",
    "print(\"Best Model Evaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_best))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_best))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_best))\n",
    "\n",
    "# Display the best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have new, unseen data stored in a DataFrame called 'new_data'\n",
    "# Make sure 'new_data' has the same features and preprocessing as your training data\n",
    "\n",
    "# Use the trained model to make predictions on the new data\n",
    "new_data_predictions = best_model.predict(new_data)\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Predictions on New Data:\")\n",
    "print(new_data_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b1a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
